[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducing example models from survivalstan",
    "section": "",
    "text": "According to Wikipedia:\n\nSurvival analysis is a branch of statistics for analyzing the expected duration of time until one event occurs, such as death in biological organisms and failure in mechanical systems.\n\nWe’ll consider the setting used for the examples at https://jburos.github.io/survivalstan/Examples.html. We will have a model which, for a set of persons, takes\n\na set of covariates (age and gender per person),\na list of times at which the event either occurs, or until which the event did not occur (one time and event/survival indicator per person),\n\nand, after following standard Bayesian procedures via conditioning on observations, yields a way to predict the survival time of unobserved persons, given the same covariates.\nFor fixed covariates \\(x\\) and model parameters \\(\\theta\\), the models below will give us a way to compute a (piecewise exponential) survival function \\(S(t) = Pr(T &gt; t)\\), i.e. a function which models the probability that the event in question has not occured until the specified time \\(t\\). Usually as well as in our setting, the survival function will be the solution to a simple linear first-order differential equation with variable coefficients, concretely we have\n\\[\nS'(t) = -\\lambda(t)S(t)\\quad\\text{and}\\quad S(0) = 1\n\\] where the hazard function/rate \\(\\lambda(t)\\) is a non-negative function, such that \\(S(t)\\) is monotonically non-increasing and has values in \\((0, 1]\\). The log of the survival function is then \\[\n\\log S(t) = -\\int_0^t\\lambda(\\tau) d\\tau.\n\\]\nAs \\(S(t)\\) models the survival (the non-occurence of an event), the log likelihood of the occurence of an event at a given time \\(t\\) is \\[\n\\log p_1(t) = \\log -S'(t) = \\log \\lambda(t) + \\log S(t) =  \\log \\lambda(t) -\\int_0^t\\lambda(\\tau) d\\tau\n\\] and the log likelihood of survival up to at least time \\(t\\) is \\[\n\\log p_0(t) = \\log S(t) = -\\int_0^t\\lambda(\\tau) d\\tau.\n\\]\nThe first term (\\(p_1(t)\\)) will have to be used for the likelihood contribution of observations of the event occuring (survival up to exactly time \\(t\\)), while the second term (\\(p_0(t)\\)) will have to be used for the likelihood contribution of observations of the event not ocurring until the end of the observation time, aka as censored observations.\nIf the hazard function \\(\\lambda(\\tau)\\) is constant and if we do not care about constant terms (as e.g. during MCMC) we can use the Poisson distribution to compute the appropriate terms “automatically”. For piecewise constant hazard functions, it’s possible to chain individual Poisson likelihoods to compute the overall likelihood (modulo a constant term).\nFor piecewise constant hazard functions of the form \\[\n\\lambda(t) = \\begin{cases}\n    \\lambda_1 & \\text{if } t \\in [t_0, t_1],\\\\\n    \\lambda_2 & \\text{if } t \\in (t_1, t_2],\\\\\n    \\dots\n\\end{cases}\n\\] with \\(0 = t_0 &lt; t_1 &lt; t_2 &lt; \\dots\\) the survival function can be directly computed as \\[\n\\log S(t_j) = -\\sum_{i=1}^j (t_i-t_{i-1}) \\lambda_i.\n\\]"
  },
  {
    "objectID": "index.html#survival-analysis---whats-that",
    "href": "index.html#survival-analysis---whats-that",
    "title": "Reproducing example models from survivalstan",
    "section": "",
    "text": "According to Wikipedia:\n\nSurvival analysis is a branch of statistics for analyzing the expected duration of time until one event occurs, such as death in biological organisms and failure in mechanical systems.\n\nWe’ll consider the setting used for the examples at https://jburos.github.io/survivalstan/Examples.html. We will have a model which, for a set of persons, takes\n\na set of covariates (age and gender per person),\na list of times at which the event either occurs, or until which the event did not occur (one time and event/survival indicator per person),\n\nand, after following standard Bayesian procedures via conditioning on observations, yields a way to predict the survival time of unobserved persons, given the same covariates.\nFor fixed covariates \\(x\\) and model parameters \\(\\theta\\), the models below will give us a way to compute a (piecewise exponential) survival function \\(S(t) = Pr(T &gt; t)\\), i.e. a function which models the probability that the event in question has not occured until the specified time \\(t\\). Usually as well as in our setting, the survival function will be the solution to a simple linear first-order differential equation with variable coefficients, concretely we have\n\\[\nS'(t) = -\\lambda(t)S(t)\\quad\\text{and}\\quad S(0) = 1\n\\] where the hazard function/rate \\(\\lambda(t)\\) is a non-negative function, such that \\(S(t)\\) is monotonically non-increasing and has values in \\((0, 1]\\). The log of the survival function is then \\[\n\\log S(t) = -\\int_0^t\\lambda(\\tau) d\\tau.\n\\]\nAs \\(S(t)\\) models the survival (the non-occurence of an event), the log likelihood of the occurence of an event at a given time \\(t\\) is \\[\n\\log p_1(t) = \\log -S'(t) = \\log \\lambda(t) + \\log S(t) =  \\log \\lambda(t) -\\int_0^t\\lambda(\\tau) d\\tau\n\\] and the log likelihood of survival up to at least time \\(t\\) is \\[\n\\log p_0(t) = \\log S(t) = -\\int_0^t\\lambda(\\tau) d\\tau.\n\\]\nThe first term (\\(p_1(t)\\)) will have to be used for the likelihood contribution of observations of the event occuring (survival up to exactly time \\(t\\)), while the second term (\\(p_0(t)\\)) will have to be used for the likelihood contribution of observations of the event not ocurring until the end of the observation time, aka as censored observations.\nIf the hazard function \\(\\lambda(\\tau)\\) is constant and if we do not care about constant terms (as e.g. during MCMC) we can use the Poisson distribution to compute the appropriate terms “automatically”. For piecewise constant hazard functions, it’s possible to chain individual Poisson likelihoods to compute the overall likelihood (modulo a constant term).\nFor piecewise constant hazard functions of the form \\[\n\\lambda(t) = \\begin{cases}\n    \\lambda_1 & \\text{if } t \\in [t_0, t_1],\\\\\n    \\lambda_2 & \\text{if } t \\in (t_1, t_2],\\\\\n    \\dots\n\\end{cases}\n\\] with \\(0 = t_0 &lt; t_1 &lt; t_2 &lt; \\dots\\) the survival function can be directly computed as \\[\n\\log S(t_j) = -\\sum_{i=1}^j (t_i-t_{i-1}) \\lambda_i.\n\\]"
  },
  {
    "objectID": "index.html#why-do-this-why-reimplement-things",
    "href": "index.html#why-do-this-why-reimplement-things",
    "title": "Reproducing example models from survivalstan",
    "section": "Why do this? Why reimplement things?",
    "text": "Why do this? Why reimplement things?\nOut of curiosity, to figure out whether. and to demonstrate that I understand survival analysis. Writing down the math is nice and all, but to get correct simulation results, every little detail has to be right. At least in principle, in practice the simulation can still be subtly wrong due to errors which don’t crash everything, but only e.g. introduce biases."
  },
  {
    "objectID": "index.html#simulation",
    "href": "index.html#simulation",
    "title": "Reproducing example models from survivalstan",
    "section": "Simulation",
    "text": "Simulation\n\n\nSimulated data\n\nTo simulate the data, we generate (for 100 persons)\n\nthe age from a Poisson distribution with mean 55,\nthe gender (male or not) from a Bernoulli distribution with mean 1/2,\nassume a constant (in time) hazard function, computed from age and male as log(hazard) = -3 + .5 * male,\ndraw true survival times true_t from an Exponential distribution with rate parameter hazard,\ncap them at a censor_time of 20, i.e. t = min(true_t, censor_time), and\nset survived to true if true_t &gt; censor_time and false otherwise.\n\n\nDataframeCode\n\n\n\n\n100×7 DataFrame75 rows omitted\n\n\n\nRow\nage\nmale\nrate\ntrue_t\nt\nsurvived\nidx\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nBool\nInt64\n\n\n\n\n1\n51\nfalse\n0.0497871\n20.672\n20.0\ntrue\n1\n\n\n2\n51\nfalse\n0.0497871\n24.6577\n20.0\ntrue\n2\n\n\n3\n50\nfalse\n0.0497871\n31.853\n20.0\ntrue\n3\n\n\n4\n55\nfalse\n0.0497871\n9.7404\n9.7404\nfalse\n4\n\n\n5\n46\nfalse\n0.0497871\n15.3396\n15.3396\nfalse\n5\n\n\n6\n45\nfalse\n0.0497871\n41.6487\n20.0\ntrue\n6\n\n\n7\n56\nfalse\n0.0497871\n12.7356\n12.7356\nfalse\n7\n\n\n8\n47\ntrue\n0.082085\n0.761362\n0.761362\nfalse\n8\n\n\n9\n61\nfalse\n0.0497871\n6.27149\n6.27149\nfalse\n9\n\n\n10\n57\nfalse\n0.0497871\n16.5202\n16.5202\nfalse\n10\n\n\n11\n54\ntrue\n0.082085\n17.3104\n17.3104\nfalse\n11\n\n\n12\n49\nfalse\n0.0497871\n0.787504\n0.787504\nfalse\n12\n\n\n13\n56\ntrue\n0.082085\n2.42708\n2.42708\nfalse\n13\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n89\n59\nfalse\n0.0497871\n21.8631\n20.0\ntrue\n89\n\n\n90\n50\ntrue\n0.082085\n3.82811\n3.82811\nfalse\n90\n\n\n91\n55\ntrue\n0.082085\n12.3331\n12.3331\nfalse\n91\n\n\n92\n56\ntrue\n0.082085\n29.3846\n20.0\ntrue\n92\n\n\n93\n47\ntrue\n0.082085\n18.4165\n18.4165\nfalse\n93\n\n\n94\n51\nfalse\n0.0497871\n25.4902\n20.0\ntrue\n94\n\n\n95\n65\nfalse\n0.0497871\n21.1051\n20.0\ntrue\n95\n\n\n96\n67\ntrue\n0.082085\n19.4895\n19.4895\nfalse\n96\n\n\n97\n62\nfalse\n0.0497871\n8.24189\n8.24189\nfalse\n97\n\n\n98\n65\nfalse\n0.0497871\n31.491\n20.0\ntrue\n98\n\n\n99\n53\ntrue\n0.082085\n6.39705\n6.39705\nfalse\n99\n\n\n100\n67\ntrue\n0.082085\n6.19111\n6.19111\nfalse\n100\n\n\n\n\n\n\n\n\nCurrently, the used formula/rate_form is hardcoded to match the examples.\nsim_data_exp_correlated(rng=Random.default_rng(); N, censor_time, rate_form, rate_coefs) = begin \n    idx = 1:N\n    age = rand(rng, Poisson(55), N)\n    male = rand(rng, Bernoulli(.5), N)\n    rate = @. exp(rate_coefs[1] + male * rate_coefs[2])\n    true_t = rand.(rng, ConstantExponentialModel.(rate))\n    t = min.(true_t, censor_time)\n    survived = true_t .&gt; censor_time\n    DataFrame((;age, male, rate, true_t, t, survived, idx))\nend\n\n\n\n\nFor all simulations,\n\nwe model the hazard function \\(\\lambda_i(t)\\) of person \\(i = 1,\\dots,100\\) to be piecewise constant, with as many pieces as there are unique event times, plus a final one which goes from the largest event observation time to the censor,\nevery person’s hazard function is unique (provided the covariates are unique),\nthe personwise (\\(i\\)) and timeslabwise (\\(j\\)) hazard values will be of the form \\[\n\\log\\lambda_{i,j} = \\log a  + \\log\\kappa_j + \\langle{}X_i,\\beta_j\\rangle{},\n\\] where \\(\\log a\\) is a scalar intercept, \\(\\log\\kappa_j\\) is a time-varying (but person-constant) effect, \\(X_i\\) are the \\(i\\)-th person’s covariates, and \\(\\beta_j\\) are the potentially time-varying covariate effects (in timeslab \\(j\\)). For the first two models, \\(\\beta\\) will be constant, while it will vary for the last model.\n\n\npem_survival_model\n\nDiscussionPosterior parameter and predictive plotsReimplemented modelOriginal model\n\n\nThe easiest model. The covariate effects are constant (\\(\\beta_1=\\beta_2=\\dots\\)) and the time-varying (but person-constant) effect \\(\\log\\kappa_j\\) has a hierarchical normal prior with mean 0 and unkown scale (with standard half-normal prior). There seems to be small mistake in the original model, where at line 42 (AFAICT) log_t_dur = log(t_obs) assign the logarithm of the event time to the variable which has to contain the logarithm of the timeslab width.\n\n\n\n\n    \n    \n\n\n\n\nfunction pem_survival_model(;\n    survived,\n    t,\n    design_matrix,\n    likelihood=true\n)\n    (;\n        n_persons, n_covariates, t1, n_timepoints, end_idxs, t0, dt, log_dts\n    ) = prepare_survival(;t, design_matrix)\n    StanBlocks.@stan begin \n        @parameters begin \n            log_hazard_intercept::real\n            beta::vector[n_covariates]\n            log_hazard_timewise_scale::real(lower=0)\n            log_hazard_timewise::vector[n_timepoints]\n        end\n        log_hazard_personwise = design_matrix*beta\n        StanBlocks.@model @views begin \n            log_hazard_intercept ~ normal(0, 1)\n            beta ~ cauchy(0, 2)\n            log_hazard_timewise_scale ~ normal(0, 1)\n            log_hazard_timewise ~ normal(0, log_hazard_timewise_scale)\n            log_lik = Base.broadcast(1:n_persons) do person \n                idxs = 1:end_idxs[person]\n                survival_lpdf(\n                    survived[person], \n                    StanBlocks.@broadcasted(log_hazard_intercept + log_hazard_personwise[person] + log_hazard_timewise[idxs]),\n                    log_dts[idxs]\n                )\n            end\n            likelihood && (target += sum(log_lik))\n        end\n        StanBlocks.@generated_quantities begin\n            log_lik = collect(log_lik)\n            t_pred = map(1:n_persons) do person \n                for timepoint in 1:n_timepoints\n                    log_hazard = log_hazard_intercept + log_hazard_personwise[person] + log_hazard_timewise[timepoint]\n                    rv = rand(Exponential(exp(-log_hazard)))\n                    rv &lt;= dt[timepoint] && return t0[timepoint] + rv\n                end\n                t1[end]\n            end\n        end\n    end\nend\n\n\n/*  Variable naming:\n // dimensions\n N          = total number of observations (length of data)\n S          = number of sample ids\n T          = max timepoint (number of timepoint ids)\n M          = number of covariates\n\n // main data matrix (per observed timepoint*record)\n s          = sample id for each obs\n t          = timepoint id for each obs\n event      = integer indicating if there was an event at time t for sample s\n x          = matrix of real-valued covariates at time t for sample n [N, X]\n\n // timepoint-specific data (per timepoint, ordered by timepoint id)\n t_obs      = observed time since origin for each timepoint id (end of period)\n t_dur      = duration of each timepoint period (first diff of t_obs)\n\n*/\n// Jacqueline Buros Novik &lt;jackinovik@gmail.com&gt;\n\ndata {\n  // dimensions\n  int&lt;lower=1&gt; N;\n  int&lt;lower=1&gt; S;\n  int&lt;lower=1&gt; T;\n  int&lt;lower=0&gt; M;\n\n  // data matrix\n  int&lt;lower=1, upper=N&gt; s[N];     // sample id\n  int&lt;lower=1, upper=T&gt; t[N];     // timepoint id\n  int&lt;lower=0, upper=1&gt; event[N]; // 1: event, 0:censor\n  matrix[N, M] x;                 // explanatory vars\n\n  // timepoint data\n  vector&lt;lower=0&gt;[T] t_obs;\n  vector&lt;lower=0&gt;[T] t_dur;\n}\ntransformed data {\n  vector[T] log_t_dur;  // log-duration for each timepoint\n  int n_trans[S, T];\n\n  log_t_dur = log(t_obs);\n\n  // n_trans used to map each sample*timepoint to n (used in gen quantities)\n  // map each patient/timepoint combination to n values\n  for (n in 1:N) {\n      n_trans[s[n], t[n]] = n;\n  }\n\n  // fill in missing values with n for max t for that patient\n  // ie assume \"last observed\" state applies forward (may be problematic for TVC)\n  // this allows us to predict failure times &gt;= observed survival times\n  for (samp in 1:S) {\n      int last_value;\n      last_value = 0;\n      for (tp in 1:T) {\n          // manual says ints are initialized to neg values\n          // so &lt;=0 is a shorthand for \"unassigned\"\n          if (n_trans[samp, tp] &lt;= 0 && last_value != 0) {\n              n_trans[samp, tp] = last_value;\n          } else {\n              last_value = n_trans[samp, tp];\n          }\n      }\n  }\n}\nparameters {\n  vector[T] log_baseline_raw; // unstructured baseline hazard for each timepoint t\n  vector[M] beta;         // beta for each covariate\n  real&lt;lower=0&gt; baseline_sigma;\n  real log_baseline_mu;\n}\ntransformed parameters {\n  vector[N] log_hazard;\n  vector[T] log_baseline;     // unstructured baseline hazard for each timepoint t\n\n  log_baseline = log_baseline_mu + log_baseline_raw + log_t_dur;\n\n  for (n in 1:N) {\n    log_hazard[n] = log_baseline[t[n]] + x[n,]*beta;\n  }\n}\nmodel {\n  beta ~ cauchy(0, 2);\n  event ~ poisson_log(log_hazard);\n  log_baseline_mu ~ normal(0, 1);\n  baseline_sigma ~ normal(0, 1);\n  log_baseline_raw ~ normal(0, baseline_sigma);\n}\ngenerated quantities {\n  real log_lik[N];\n  vector[T] baseline;\n  real y_hat_time[S];      // predicted failure time for each sample\n  int y_hat_event[S];      // predicted event (0:censor, 1:event)\n\n  // compute raw baseline hazard, for summary/plotting\n  baseline = exp(log_baseline_mu + log_baseline_raw);\n\n  // prepare log_lik for loo-psis\n  for (n in 1:N) {\n      log_lik[n] = poisson_log_log(event[n], log_hazard[n]);\n  }\n\n  // posterior predicted values\n  for (samp in 1:S) {\n      int sample_alive;\n      sample_alive = 1;\n      for (tp in 1:T) {\n        if (sample_alive == 1) {\n              int n;\n              int pred_y;\n              real log_haz;\n\n              // determine predicted value of this sample's hazard\n              n = n_trans[samp, tp];\n              log_haz = log_baseline[tp] + x[n,] * beta;\n\n              // now, make posterior prediction of an event at this tp\n              if (log_haz &lt; log(pow(2, 30)))\n                  pred_y = poisson_log_rng(log_haz);\n              else\n                  pred_y = 9;\n\n              // summarize survival time (observed) for this pt\n              if (pred_y &gt;= 1) {\n                  // mark this patient as ineligible for future tps\n                  // note: deliberately treat 9s as events\n                  sample_alive = 0;\n                  y_hat_time[samp] = t_obs[tp];\n                  y_hat_event[samp] = 1;\n              }\n\n          }\n      } // end per-timepoint loop\n\n      // if patient still alive at max\n      if (sample_alive == 1) {\n          y_hat_time[samp] = t_obs[T];\n          y_hat_event[samp] = 0;\n      }\n  } // end per-sample loop\n}\n\n\n\n\n\npem_survival_model_randomwalk\n\nDiscussionPosterior parameter and predictive plotsReimplemented modelOriginal model\n\n\nIdentical to the first model, except that the time-varying (but person-constant) effect \\(\\log\\kappa_j\\) should have a “random walk” prior. AFAICT, the original model has the same small mistake as the first one (this time at line 43), but IMO some (minor) other things goes “wrong” in constructing the “random walk” prior, or rather, I believe that instead of a random walk prior as implemented in the original code, an approximate Brownian motion / Wiener process prior would have been a better choice:\nA random walk prior as implemented in the original code will imply different priors for different numbers of persons and also for different realizations of the event times, while an approximate Wiener process prior does not (or rather, much less). Consider the following:\n\n(Gaussian) random walk prior\nFor random walk parameters \\(x_1, x_2, \\dots\\) with scale parameter \\(\\sigma\\), the (conditional) prior density is \\[\n    p(x_i | x_{i-1}) = p_\\mathcal{N}(x_i | x_{i-1}, \\sigma^2) \\text{ for } i=1,2,\\dots\n\\] and with \\(x_0\\) another parameter with appropriate prior.\n\n\nApproximate (Gaussian) Wiener process prior\nFollowing Wikipedia:\n\nThe Wiener process \\(W_t\\) is characterised by the following properties: […] W has Gaussian increments: […] \\(W_{t+i} - W_t \\sim \\mathcal{N}(0,u)\\).\n\nI.e., for timepoints \\(0 = t_0 &lt; t_1 &lt; t_2 &lt; \\dots\\) as above, the (conditional) prior density of the (shifted) Wiener process values \\(x_1, x_2, \\dots\\) with scale parameter \\(\\sigma\\) is \\[\n    p(x_i | x_{i-1}) = p_\\mathcal{N}(x_i | x_{i-1}, (t_i-t_{i-1})\\sigma^2) \\text{ for } i=1,2,\\dots\n\\] and with \\(x_0\\) as before.\n\n\nDependence on the observed event times\nThe difference between the two priors will become most easily apparent by looking at the implied prior on the (log) hazard at (or right before) the censor time \\(t_\\text{censor} = t_{N+1}\\), for varying numbers of unique observed event times \\(N\\). For the random walk prior, we’ll have \\[\nx_j \\sim \\mathcal{N}(x_0, j\\sigma^2) \\text{ for } j = 1,\\dots,N+1,\n\\] while for the Wiener process prior, we’ll have \\[\nx_j \\sim \\mathcal{N}(0, t_j\\sigma^2) \\text{ for } j = 1,\\dots,N+1.\n\\] In particular, for \\(j=N+1\\) (i.e. at censor time), we get a constant prior distribution for the Wiener process prior, but for the random walk prior we get a prior distribution that depends on the number of unique observed event times \\(N\\). Similarly, even for fixed \\(N\\), there is a (potentially strong) dependence of the implied prior for “interior” time slabs on the realization of the even times for the random walk prior, while there’s “no” dependence of the implied prior for the Wiener process prior. Caveat: There will actually be a dependence of the implied prior on the event time realizations also for the Wiener process, but this is only due to the piecewise-constant “assumption” and can be interpreted as an approximation error to the solution of the underlying stochastic differential equation.\n\n\n\n\n\n    \n    \n\n\n\n\nfunction pem_survival_model_randomwalk(;\n    survived,\n    t,\n    design_matrix,\n    likelihood=true\n)\n    (;\n        n_persons, n_covariates, t1, n_timepoints, end_idxs, t0, dt, log_dts\n    ) = prepare_survival(;t, design_matrix)\n    rw_sqrt_scale = @. sqrt(.5*(dt[1:end-1] + dt[2:end]))\n    StanBlocks.@stan begin \n        @parameters begin \n            log_hazard_intercept::real\n            beta::vector[n_covariates]\n            log_hazard_timewise_scale::real(lower=0)\n            log_hazard_timewise::vector[n_timepoints]\n        end\n        log_hazard_personwise = design_matrix*beta\n        StanBlocks.@model @views begin \n            log_hazard_intercept ~ normal(0, 1)\n            beta ~ cauchy(0, 2)\n            log_hazard_timewise_scale ~ normal(0, 1)\n            log_hazard_timewise[1] ~ normal(0, 1)\n            log_hazard_timewise ~ random_walk(\n                StanBlocks.@broadcasted(log_hazard_timewise_scale * rw_sqrt_scale)\n            )\n            log_lik = Base.broadcast(1:n_persons) do person \n                idxs = 1:end_idxs[person]\n                survival_lpdf(\n                    survived[person], \n                    StanBlocks.@broadcasted(log_hazard_intercept + log_hazard_personwise[person] + log_hazard_timewise[idxs]),\n                    log_dts[idxs]\n                )\n            end\n            likelihood && (target += sum(log_lik))\n        end\n        StanBlocks.@generated_quantities begin\n            log_lik = collect(log_lik)\n            t_pred = map(1:n_persons) do person \n                for timepoint in 1:n_timepoints\n                    log_hazard = log_hazard_intercept + log_hazard_personwise[person] + log_hazard_timewise[timepoint]\n                    rv = rand(Exponential(exp(-log_hazard)))\n                    rv &lt;= dt[timepoint] && return t0[timepoint] + rv\n                end\n                t1[end]\n            end\n        end\n    end\nend\n\n\n/*  Variable naming:\n // dimensions\n N          = total number of observations (length of data)\n S          = number of sample ids\n T          = max timepoint (number of timepoint ids)\n M          = number of covariates\n\n // main data matrix (per observed timepoint*record)\n s          = sample id for each obs\n t          = timepoint id for each obs\n event      = integer indicating if there was an event at time t for sample s\n x          = matrix of real-valued covariates at time t for sample n [N, X]\n\n // timepoint-specific data (per timepoint, ordered by timepoint id)\n t_obs      = observed time since origin for each timepoint id (end of period)\n t_dur      = duration of each timepoint period (first diff of t_obs)\n\n*/\n// Jacqueline Buros Novik &lt;jackinovik@gmail.com&gt;\n\n\ndata {\n  // dimensions\n  int&lt;lower=1&gt; N;\n  int&lt;lower=1&gt; S;\n  int&lt;lower=1&gt; T;\n  int&lt;lower=0&gt; M;\n\n  // data matrix\n  int&lt;lower=1, upper=N&gt; s[N];     // sample id\n  int&lt;lower=1, upper=T&gt; t[N];     // timepoint id\n  int&lt;lower=0, upper=1&gt; event[N]; // 1: event, 0:censor\n  matrix[N, M] x;                 // explanatory vars\n\n  // timepoint data\n  vector&lt;lower=0&gt;[T] t_obs;\n  vector&lt;lower=0&gt;[T] t_dur;\n}\ntransformed data {\n  vector[T] log_t_dur;  // log-duration for each timepoint\n  int n_trans[S, T];\n\n  log_t_dur = log(t_obs);\n\n  // n_trans used to map each sample*timepoint to n (used in gen quantities)\n  // map each patient/timepoint combination to n values\n  for (n in 1:N) {\n      n_trans[s[n], t[n]] = n;\n  }\n\n  // fill in missing values with n for max t for that patient\n  // ie assume \"last observed\" state applies forward (may be problematic for TVC)\n  // this allows us to predict failure times &gt;= observed survival times\n  for (samp in 1:S) {\n      int last_value;\n      last_value = 0;\n      for (tp in 1:T) {\n          // manual says ints are initialized to neg values\n          // so &lt;=0 is a shorthand for \"unassigned\"\n          if (n_trans[samp, tp] &lt;= 0 && last_value != 0) {\n              n_trans[samp, tp] = last_value;\n          } else {\n              last_value = n_trans[samp, tp];\n          }\n      }\n  }\n}\nparameters {\n  vector[T] log_baseline_raw; // unstructured baseline hazard for each timepoint t\n  vector[M] beta;                      // beta for each covariate\n  real&lt;lower=0&gt; baseline_sigma;\n  real log_baseline_mu;\n}\ntransformed parameters {\n  vector[N] log_hazard;\n  vector[T] log_baseline;\n\n  log_baseline = log_baseline_raw + log_t_dur;\n\n  for (n in 1:N) {\n    log_hazard[n] = log_baseline_mu + log_baseline[t[n]] + x[n,]*beta;\n  }\n}\nmodel {\n  beta ~ cauchy(0, 2);\n  event ~ poisson_log(log_hazard);\n  log_baseline_mu ~ normal(0, 1);\n  baseline_sigma ~ normal(0, 1);\n  log_baseline_raw[1] ~ normal(0, 1);\n  for (i in 2:T) {\n      log_baseline_raw[i] ~ normal(log_baseline_raw[i-1], baseline_sigma);\n  }\n}\ngenerated quantities {\n  real log_lik[N];\n  vector[T] baseline;\n  int y_hat_mat[S, T];     // ppcheck for each S*T combination\n  real y_hat_time[S];      // predicted failure time for each sample\n  int y_hat_event[S];      // predicted event (0:censor, 1:event)\n\n  // compute raw baseline hazard, for summary/plotting\n  baseline = exp(log_baseline_raw);\n\n  for (n in 1:N) {\n      log_lik[n] &lt;- poisson_log_lpmf(event[n] | log_hazard[n]);\n  }\n\n  // posterior predicted values\n  for (samp in 1:S) {\n      int sample_alive;\n      sample_alive = 1;\n      for (tp in 1:T) {\n        if (sample_alive == 1) {\n              int n;\n              int pred_y;\n              real log_haz;\n\n              // determine predicted value of y\n              // (need to recalc so that carried-forward data use sim tp and not t[n])\n              n = n_trans[samp, tp];\n              log_haz = log_baseline_mu + log_baseline[tp] + x[n,]*beta;\n              if (log_haz &lt; log(pow(2, 30)))\n                  pred_y = poisson_log_rng(log_haz);\n              else\n                  pred_y = 9;\n\n              // mark this patient as ineligible for future tps\n              // note: deliberately make 9s ineligible\n              if (pred_y &gt;= 1) {\n                  sample_alive = 0;\n                  y_hat_time[samp] = t_obs[tp];\n                  y_hat_event[samp] = 1;\n              }\n\n              // save predicted value of y to matrix\n              y_hat_mat[samp, tp] = pred_y;\n          }\n          else if (sample_alive == 0) {\n              y_hat_mat[samp, tp] = 9;\n          }\n      } // end per-timepoint loop\n\n      // if patient still alive at max\n      //\n      if (sample_alive == 1) {\n          y_hat_time[samp] = t_obs[T];\n          y_hat_event[samp] = 0;\n      }\n  } // end per-sample loop\n}\n\n\n\n\n\npem_survival_model_timevarying\n\nDiscussionPosterior parameter and predictive plotsReimplemented modelOriginal model\n\n\nTo be finished. To keep things short:\n\nThe original model has the same minor problems as the other models.\nWhile the original model implements a random walk prior on the increments of the covariate effects, I’ve kept things a bit simpler and instead just implemented the corresponding Wiener process prior on the values of the covariate effects. IMO, putting a given prior on the increments instead of on the values or vice versa is a modeling decision, and not a “mistake” by any stretch of the imagination. Doing one or the other implies different things, and which choice is “better” is not clear a priori and may depend on the setting.\nI believe sampling may have failed a bit for the run included in this notebook. I believe I have seen better sampling “runs”, but as this doesn’t have to be perfect, I’ve left it as is.\n\n\n\n\n\n    \n    \n\n\n\n\nfunction pem_survival_model_timevarying(;\n    survived,\n    t,\n    design_matrix,\n    likelihood=true\n)\n    (;\n        n_persons, n_covariates, t1, n_timepoints, end_idxs, t0, dt, log_dts\n    ) = prepare_survival(;t, design_matrix)\n    rw_sqrt_scale = @. sqrt(.5*(dt[1:end-1] + dt[2:end]))\n    StanBlocks.@stan begin \n        @parameters begin \n            log_hazard_intercept::real\n            beta_timewise_scale::real(lower=0)\n            beta_timewise::matrix[n_covariates, n_timepoints]\n            log_hazard_timewise_scale::real(lower=0)\n            log_hazard_timewise::vector[n_timepoints]\n        end\n        log_hazard_personwise = design_matrix*beta_timewise\n        StanBlocks.@model @views begin \n            log_hazard_intercept ~ normal(0, 1)\n            beta_timewise_scale ~ cauchy(0, 1)\n            beta_timewise[:, 1] ~ cauchy(0, 1) \n            beta_timewise' ~ random_walk(\n                StanBlocks.@broadcasted(beta_timewise_scale * rw_sqrt_scale)\n            )\n            log_hazard_timewise_scale ~ normal(0, 1)\n            log_hazard_timewise[1] ~ normal(0, 1)\n            log_hazard_timewise ~ random_walk(\n                StanBlocks.@broadcasted(log_hazard_timewise_scale * rw_sqrt_scale)\n            )\n            log_lik = Base.broadcast(1:n_persons) do person \n                idxs = 1:end_idxs[person]\n                survival_lpdf(\n                    survived[person], \n                    StanBlocks.@broadcasted(log_hazard_intercept + log_hazard_personwise[person, idxs] + log_hazard_timewise[idxs]),\n                    log_dts[idxs]\n                )\n            end\n            likelihood && (target += sum(log_lik))\n        end\n        StanBlocks.@generated_quantities begin\n            log_lik = collect(log_lik)\n            t_pred = map(1:n_persons) do person \n                for timepoint in 1:n_timepoints\n                    log_hazard = log_hazard_intercept + log_hazard_personwise[person, timepoint] + log_hazard_timewise[timepoint]\n                    rv = rand(Exponential(exp(-log_hazard)))\n                    rv &lt;= dt[timepoint] && return t0[timepoint] + rv\n                end\n                t1[end]\n            end\n        end\n    end\nend\n\n\n/*  Variable naming:\n // dimensions\n N          = total number of observations (length of data)\n S          = number of sample ids\n T          = max timepoint (number of timepoint ids)\n M          = number of covariates\n\n // data\n s          = sample id for each obs\n t          = timepoint id for each obs\n event      = integer indicating if there was an event at time t for sample s\n x          = matrix of real-valued covariates at time t for sample n [N, X]\n obs_t      = observed end time for interval for timepoint for that obs\n\n*/\n// Jacqueline Buros Novik &lt;jackinovik@gmail.com&gt;\n\nfunctions {\n  matrix spline(vector x, int N, int H, vector xi, int P) {\n    matrix[N, H + P] b_x;         // expanded predictors\n    for (n in 1:N) {\n        for (p in 1:P) {\n            b_x[n,p] &lt;- pow(x[n],p-1);  // x[n]^(p-1)\n        }\n        for (h in 1:H)\n          b_x[n, h + P] &lt;- fmax(0, pow(x[n] - xi[h],P-1));\n    }\n    return b_x;\n  }\n}\ndata {\n  // dimensions\n  int&lt;lower=1&gt; N;\n  int&lt;lower=1&gt; S;\n  int&lt;lower=1&gt; T;\n  int&lt;lower=0&gt; M;\n\n  // data matrix\n  int&lt;lower=1, upper=N&gt; s[N];     // sample id\n  int&lt;lower=1, upper=T&gt; t[N];     // timepoint id\n  int&lt;lower=0, upper=1&gt; event[N]; // 1: event, 0:censor\n  matrix[N, M] x;                 // explanatory vars\n\n  // timepoint data\n  vector&lt;lower=0&gt;[T] t_obs;\n  vector&lt;lower=0&gt;[T] t_dur;\n}\ntransformed data {\n  vector[T] log_t_dur;\n  int n_trans[S, T];\n\n  log_t_dur = log(t_obs);\n\n  // n_trans used to map each sample*timepoint to n (used in gen quantities)\n  // map each patient/timepoint combination to n values\n  for (n in 1:N) {\n      n_trans[s[n], t[n]] = n;\n  }\n\n  // fill in missing values with n for max t for that patient\n  // ie assume \"last observed\" state applies forward (may be problematic for TVC)\n  // this allows us to predict failure times &gt;= observed survival times\n  for (samp in 1:S) {\n      int last_value;\n      last_value = 0;\n      for (tp in 1:T) {\n          // manual says ints are initialized to neg values\n          // so &lt;=0 is a shorthand for \"unassigned\"\n          if (n_trans[samp, tp] &lt;= 0 && last_value != 0) {\n              n_trans[samp, tp] = last_value;\n          } else {\n              last_value = n_trans[samp, tp];\n          }\n      }\n  }\n}\nparameters {\n  vector[T] log_baseline_raw;    // unstructured baseline hazard for each timepoint t\n  real&lt;lower=0&gt; baseline_sigma;\n  real log_baseline_mu;\n\n  vector[M] beta; // beta-intercept\n  vector&lt;lower=0&gt;[M] beta_time_sigma;\n  vector[T-1] raw_beta_time_deltas[M]; // for each coefficient\n                                       // change in coefficient value from previous time\n}\ntransformed parameters {\n  vector[N] log_hazard;\n  vector[T] log_baseline;\n  vector[T] beta_time[M];\n  vector[T] beta_time_deltas[M];\n\n  // adjust baseline hazard for duration of each period\n  log_baseline = log_baseline_raw + log_t_dur;\n\n  // compute timepoint-specific betas\n  // offsets from previous time\n  for (coef in 1:M) {\n      beta_time_deltas[coef][1] = 0;\n      for (time in 2:T) {\n          beta_time_deltas[coef][time] = raw_beta_time_deltas[coef][time-1];\n      }\n  }\n\n  // coefficients for each timepoint T\n  for (coef in 1:M) {\n      beta_time[coef] = beta[coef] + cumulative_sum(beta_time_deltas[coef]);\n  }\n\n  // compute log-hazard for each obs\n  for (n in 1:N) {\n    real log_linpred;\n    log_linpred &lt;- 0;\n    for (coef in 1:M) {\n      // for now, handle each coef separately\n      // (to be sure we pull out the \"right\" beta..)\n      log_linpred = log_linpred + x[n, coef] * beta_time[coef][t[n]];\n    }\n    log_hazard[n] = log_baseline_mu + log_baseline[t[n]] + log_linpred;\n  }\n}\nmodel {\n  // priors on time-varying coefficients\n  for (m in 1:M) {\n    raw_beta_time_deltas[m][1] ~ normal(0, 100);\n    for(i in 2:(T-1)){\n        raw_beta_time_deltas[m][i] ~ normal(raw_beta_time_deltas[m][i-1], beta_time_sigma[m]);\n    }\n  }\n  beta_time_sigma ~ cauchy(0, 1);\n  beta ~ cauchy(0, 1);\n\n  // priors on baseline hazard\n  log_baseline_mu ~ normal(0, 1);\n  baseline_sigma ~ normal(0, 1);\n  log_baseline_raw[1] ~ normal(0, 1);\n  for (i in 2:T) {\n      log_baseline_raw[i] ~ normal(log_baseline_raw[i-1], baseline_sigma);\n  }\n\n  // model\n  event ~ poisson_log(log_hazard);\n}\ngenerated quantities {\n  real log_lik[N];\n  vector[T] baseline;\n  int y_hat_mat[S, T];     // ppcheck for each S*T combination\n  real y_hat_time[S];      // predicted failure time for each sample\n  int y_hat_event[S];      // predicted event (0:censor, 1:event)\n\n  // compute raw baseline hazard, for summary/plotting\n  baseline = exp(log_baseline_raw);\n\n  // log_likelihood for loo-psis\n  for (n in 1:N) {\n      log_lik[n] &lt;- poisson_log_lpmf(event[n] | log_hazard[n]);\n  }\n\n  // posterior predicted values\n  for (samp in 1:S) {\n      int sample_alive;\n      sample_alive = 1;\n      for (tp in 1:T) {\n        if (sample_alive == 1) {\n              int n;\n              int pred_y;\n              real log_linpred;\n              real log_haz;\n\n              // determine predicted value of y\n              n = n_trans[samp, tp];\n\n              // (borrow code from above to calc linpred)\n              // but use sim tp not t[n]\n              log_linpred = 0;\n              for (coef in 1:M) {\n                  // for now, handle each coef separately\n                  // (to be sure we pull out the \"right\" beta..)\n                  log_linpred = log_linpred + x[n, coef] * beta_time[coef][tp];\n              }\n              log_haz = log_baseline_mu + log_baseline[tp] + log_linpred;\n\n              // now, make posterior prediction\n              if (log_haz &lt; log(pow(2, 30)))\n                  pred_y = poisson_log_rng(log_haz);\n              else\n                  pred_y = 9;\n\n              // mark this patient as ineligible for future tps\n              // note: deliberately make 9s ineligible\n              if (pred_y &gt;= 1) {\n                  sample_alive = 0;\n                  y_hat_time[samp] = t_obs[tp];\n                  y_hat_event[samp] = 1;\n              }\n\n              // save predicted value of y to matrix\n              y_hat_mat[samp, tp] = pred_y;\n          }\n          else if (sample_alive == 0) {\n              y_hat_mat[samp, tp] = 9;\n          }\n      } // end per-timepoint loop\n\n      // if patient still alive at max\n      //\n      if (sample_alive == 1) {\n          y_hat_time[samp] = t_obs[T];\n          y_hat_event[samp] = 0;\n      }\n  } // end per-sample loop\n}"
  },
  {
    "objectID": "index.html#addendum-disclaimer",
    "href": "index.html#addendum-disclaimer",
    "title": "Reproducing example models from survivalstan",
    "section": "Addendum / Disclaimer",
    "text": "Addendum / Disclaimer\n\nI am aware that survivalstan hasn’t been updated in the last 7 years (according to github). I have not implemented the above models to unearth any errors or write a competitor. I believe but haven’t checked, that the “actual” models used by survivalstan are “more” correct. I was mainly curious whether I could do it, and I wanted to see how well StanBlocks.jl does.\nI’ve skipped the pem_survival_model_gamma model showcased at https://jburos.github.io/survivalstan/examples/Test%20pem_survival_model_gamma%20with%20simulated%20data.html because I did not understand why the widths of the timeslabs should affect the shape parameter of the Gamma prior. Only after implementing the time varying models did I discover the models at https://nbviewer.org/github/hammerlab/survivalstan/blob/master/example-notebooks/Test%20new_gamma_survival_model%20with%20simulated%20data.ipynb. Also, the “Worked examples” page lists a “User-supplied PEM survival model with gammahazard”, though for some reason it does not show up in the sidebar for either of the other examples, compare https://jburos.github.io/survivalstan/examples/Example-using-pem_survival_model.html, https://jburos.github.io/survivalstan/examples/Test%20pem_survival_model_gamma%20with%20simulated%20data.html, https://jburos.github.io/survivalstan/examples/Test%20pem_survival_model_randomwalk%20with%20simulated%20data.html and https://jburos.github.io/survivalstan/examples/Test%20pem_survival_model_timevarying%20with%20simulated%20data.html."
  }
]